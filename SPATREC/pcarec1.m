function [recout,cvout]=pcarec1(kmode,datin,setmodel,specs,Snames)
% pcarec1:  reconstruction by PCR, with cross-validation
% CALL: [recout,cvout]=pcarec1(kmode,datin,specs);
%
% D Meko 10-18-97
%
%
%*************** IN ARGS **********************************
% 
% kmode  cross-validation (cv) or reconstruction mode
%      ==1 reconstruction-1: automatically fits 'reasonable' model  
%      ==2 exploratory cross-validation; points out possibly better model
%      ==3 reconstruction-2: final fitting of specified model
%
% datin{}
%  {1} X (mm x p)r  tsm of calib-pd predictors. These time series are assumed to have
%		been generated by reglag1.m such that the series-to-series differences
%		in variance are meaningful.  Thus subsequent PCA will be on the
%		covariance matrix. mm is the length of the calibration period
%  {2} y (mm x 1)r cv time series of calib-pd predictand
%  {3} X1 (mX1 x p)r tsm of predictors for long-term reconstruction
%
% setmodel{}  user-specified model specifications
% {1} nkeep(1 x 1)i  retain exactly this number of PC's  in reducing predictors
%    (see kopt(1))
% {2} npred(1 x 1)i  use exactly this number of PCs as predictors (see kopt(3))
%
% specs{} model specifications
%  {1} mineigs (1 x 1)i min number of predictor variables for
%		which eigenvalue discarding is used
%  {2} fract1 (1 x 1)r fraction proportion of variance at which to cut off
%		retention of predictor PCs (see kopt(1))
%  {3} tcrit (1 x 1)r significance level (say, 0.95) for
%	  including predictors in regression model
%  {4} kopt(1 x3)i options
%		kopt(1)-- retention of important predictor eigs
%			1 -- retain all
%			2 -- retain those whose eigenvalue larger than average eigenvalue
%			3 -- retain enough PC's to explain dec fraction fract1 of variance
%		kopt(2) -- reserved
%		kopt(3) -- predictor selection option
%			1 -- use all predictors
%			2 -- only those whose reg coeffs sig at tcrit level (e.g., .95)
% Snames -- string matrix of site info
%******************  OUT ARGS ******************************
%
% recout{} reconstruction data (if kmode~=1; otherwise, empty)
%  {1} yhat1 (mm x 1)r reconstructed predictand for calib period
%  {2} yhat2 (mX1 x 1)r reconstructed predictand for data X1
%  {3} rsq (1 x 1)r decimal fraction of explained variance
%  {4) rmse (1 x 2)r for calibration and validation
%  {5} RE (1 x 1)r for validation
%  {6} sest (1 x 1)r standard error or estimate
%  {7} nv(1 x 3)i number of variables:
%    1 - p=full number of predictor eigenvectors; equivalent to number of
%		original predictor variables
%    2 - pp=number of potential predictors after discarding eigenvectors
%    3 - number of predictors actually used after t-test deletion
%  {8} hf(mX1 x q)L  extrapolation (1) vs interpolation(0) flag for
%		reconstructed data yhat2
%  {9} summary predictor PC info -- only for those PC's used in the eqn
%      {1} (mE1 x nuse)r  eigenvectors (each col an eigenvector)
%      {2} (1 x nuse)i which PCs used
%      {3} (1 x nuse)r the regression weights on the PCs
%      {4} (1 x nuse)r percent variance of predictand accounted for by PC
%  {10} Ecv -- cross validation residuals
% cvout{} information on results of cross validation
%  {1} nk (1 x 1)i number of predictor PC's corresponding to fract1 
%  {2} S1 (nk x nk)r  rmse for various combinations of PC's retained
%     and retained PC's used as predictors 
%  {3} S2 (nk x nk)r likewise, but for reduction of error statistic
% 
%****************** USER-WRITTEN FUNCTIONS CALLED ***********
%
% eigen1.m -- PCA on predictors and predictands
% mce1.m -- minimum covering ellipsoid to flag extrapolations
% pcareg1.m -- compute the PCR coefficients
%
%********************  NOTES ******************************
%
%
% t-test for significance of PC-regression coefficients: 
% Mardia, K. V., Kent, J. T, and Bibby, J. M., 1979.  Multivariate
% Analysis.  Academic Press Limited, 518 pp.  See p. 244-246.
%
% Interpolation vs extrapolation:
% Weisberg, Sanford, 1985.  Applied Linear Regression.  
% John Wiley & Sons, 324 pp.  See p. 235-237.
%
%
% STEPS
% PCA on full set of available X variables
% If cross-validation mode
%		-Determine number of X PCs corresponding to fract1 -->nkeep
%		-Loop over retained PCs nnp = 1 to nkeep
%			-Loop over number of predictors 1 to nnp
%				-Compute and store cross-validation stats
%			-end loop
%		-end loop
% Elseif reconstruction mode
%		If reconstruction mode 1
%			-Reduce X data via specified nkeep
%			-Regress using specified number of predictors npred
% 	Elseif reconstruction mode 2
%			-Using kopt(1) and associated info, reduce X data via PCA
%			-Using kopt(3) and associated info, build regression model
%		End
%		Reconstruct
%		Store regression and recon stats
%	End
%
%


%******************** UNLOAD CELL INPUT

%-------- datin
X=datin{1};
y=datin{2};
X1=datin{3};

%-------- setmodel
nkeep = setmodel{1};
npred = setmodel{2};

%--------- specs
mineigs = specs{1};
fract1 = specs{2};
tcrit = specs{3};
kopt = specs{4};

%******************  Initialize, size, preallocate *************

[mm,p]=size(X); 
[dum1,ny]=size(y);
if dum1~=mm;
	error('X and y must be same row-size');
end
if ny~=1;
	error('y must be column vector');
end
clear dum1 ny


[mX1,nX1]=size(X1);
if nX1~=p,
	error('X1 must have save col size as X')
end


%************  Compute means and standard deviations
Xmean=mean(X);
ymean=mean(y);
Xstd = std(X);
ystd = std(y);


% Expand mean and std deviation of predictand into col vectors
yym = ymean(ones(mm,1),:);
yymlong = ymean(ones(mX1,1),:); % 
yys = ystd(ones(mm,1),:);

% Expand rv of predictor means to matrices same row size as X1 and X
X1MN = repmat(Xmean,mX1,1);
XMN = repmat(Xmean,mm,1);


%****  'center' predictand and predictor data  (see Marsdia, p. 244, eqn 8.8.1
Xcent = X - XMN;
X1cent = X1 - X1MN;
ycent = y -yym;


%******************  PCA for data reduction ***************
%
% Critical to run the PCA on the covariance matrix
%

[RE,E,LE,SE,U,CE]=eigen1(Xcent,2); % PCA on covariance matrix of X


if kmode==2; % exploratory cross-validation mode
   % Use fract1 to determine number of PCS (nkeep) to retain
   
   sumev = sum(diag(LE)); % sum of eigenvalues of covariance matrix
   varfract= diag(LE)/sumev; % decimal fraction of variation explaine by each
   varcum= cumsum(varfract); % cumulative prop of variance explained
   pp = min(find(varcum>=fract1));
   nkeep= max([pp    min([p mineigs])   ]);
   [V1,V2]=crossv2(X,y,nkeep);
   cvout{1}=nkeep;
   cvout{2}=V1;
   cvout{3}=V2;
   recout=[];
   return;


else; %  one of two possible recon modes
   
   cvout=[];

	% Compute number of PCs to retain, pp
	if kmode==3; % recon mode with specified nkeep and npred
		pp = nkeep;
	elseif kmode==1 ; % recon mode without specified nkeep and npred
		if kopt(1)==1; % use all predictors
			pp=p;
		elseif kopt(1)==2; % eigenvalue of 1 criterion.  When using the covariance
  		% matrix, this translates to excluding any eigenvalues smaller than the average 
	   	% eigenvale.  Also called Kaiser's criterion (Mardia et al. 1979, p. 225)
   		if p>mineigs; % Will want to reduce data set
     		mneigval = mean(diag(LE)); % average eigenvalue
				pp =  max([sum(diag(LE)>mneigval) mineigs]);
			else; % Too few PC's to worry about reducing data set; use all PC's.
				pp=p;
			end
			
		elseif kopt(1)==3; % cutoff by pctg of predictor variance 
   		% Total variance is proportional to sum of the eigenvalues.  Percentage
   		% of total variation explained by a PC is equal to ration of the 
   		% PC's eigenvalue to the sum of the eigenvalues
   		sumev = sum(diag(LE));
   		varfract= diag(LE)/sumev; % decimal fraction of variation explaine by each
   		varcum= cumsum(varfract); % cumulative prop of variance explained
   		pp = min(find(varcum>=fract1));
   		pp= max([pp    min([p mineigs])   ]);
		end
		nkeep=pp;
	end; % of if kmode==3


	% Reduce the matrices of PC scores and eigenvalues
	U = U(:,1:pp); % PC scores, reduced set
	LEsub = diag(LE);  
	LEsub = LEsub(1:pp);
	LEpp = LE(1:pp,1:pp); % eigenvalues, reduced set


	%**************  Estimate regression coefficients
	if kmode==3; % specified number of predictors
		atilde = inv(U'*U)*U'*ycent; % with all predictors in model
      % Must change all except largest npred of atilde to zero
      % t-value corresponding to element of atilde is proportional to 
      % atilde times the square root of (sample size times eigenvalue)
      % Thus make a variable asize proportional to the 'importance'
      % of each element of atilde
      asize = atilde .* sqrt(mm*LEsub);
		[s,js]=sort(abs(asize)); % sorted smallest to largest
		nzero = length(atilde)-npred; % change this many to zero
		jzero = js(1:nzero);
		atilde(jzero)=0;
	elseif kmode==1; % must determine number of predictors
		if kopt(3)==1; % skip the t-test, use all potential predictors
			atilde = inv(U'*U)*U'*ycent;
		elseif kopt(3)==2; % sig test on coefficients -- use 95%
  		[atilde,t] = pcareg1(LEsub,U,ycent,E(:,1:pp),tcrit);
		end; % of if kopt(3)
		npred=sum(atilde~=0);
	end ; % of if kmode==3
      
	B = E(:,1:pp)* atilde; % regression coefs on the original predictor variables
	nused = sum(atilde~=0);

	nv = [p pp nused];


	% ***** Proportion of variation explained ( see ewn 8.8.6 in Mardia et al. 1979)

	rsqeach = (mm * LEsub .* (atilde .* atilde)) / (ycent' * ycent) ; % cv of R-squared each component
	rsq=sum(rsqeach);  % total proportion of variance explained
   
   % Gather some PC info to be passed back to calling program 
   L5 = atilde~=0;  % point to which of pp eigenvectors used as predictors
      % predictor PC in the regression equation
   Edud = E(:,1:pp);
   Edud = Edud(:,L5); % The eigenvector weights
   recout{9}{1}=Edud; % the eigenvector weights
   recout{9}{2}=(find(L5))'; % sequence # of PC
   recout{9}{3}=(atilde(L5))'; % weight on PC as predictor
   recout{9}{4}=(rsqeach(L5))'; % pctg variance of predictand explained by PC
   
   %---------- Sting matrix with sites used as predictors
   
   nlen1 = size(Snames,2);
   fmte = '%8.4f';
   Sn=[];
   for mdud = 1:p; % loop over Tree-ring sites
      ss = sprintf(fmte,Edud(mdud,:));
      Sn=[Sn; [Snames(mdud,:) ss]];
   end
   
   
	%************  Calibration period predicted values

	yhat1 = U*atilde + yym;

	%***************** Root mean square error
   [dummy,rmse,redummy]=rederr(ymean,ymean,yhat1,y);
   rmse=rmse(1);
	


	%********************* Reconstruction-period predicted amplitudes


	yhat2 = X1cent*B + yymlong;



	%*****  Interpolation vs Extrapolation *************************
	%
	% Calibration-period predictor data for the regression is U, 
	% which is zero-mean. Let the corresponding transformed data for the
	% years outside the calibration period be U1:
	%
	% Get used subset of columns of E
	% Find out which of the pp predictor amplitudes were used in any of the
	% regressions to predict y
	j1 = atilde~=0;  % logical col vector to used cols of E(:,1:pp)
	E1 = E(:,1:pp);
	E1 = E1(:,j1);

	U1 = X1*E1;

	% Get matrix of used calibration-period predictor eig-amps
	U2 = U(:,j1);

	% Matrix of corresponding data outside the calibration period
	% is already U1 

	% Stack non-calibration and calibration predictor data 
	U3=[U1;U2];

	% The call for minimum covering ellipsoid is
	% [S,hmax]=mce1(X,yrs1,yrs2), where X is U3, yrs1 are start and end
	% year for X, and yrs2 are for calibration period
	yrs1=[1 (mX1+mm)];
	yrs2=[(mX1+1) (mX1+mm)];
	[S,hmax]= mce1(U3,yrs1,yrs2);
	hf=S(1:mX1,3); % flag for outside-calibration-period years
		% 1 = extrapolation, 0 =interpolation

	disp('Starting Cross-Validation');
	%---------- CROSS VALIDATE BY LEAVE-1-OUT
	[v1,v2,v3]=crossv1(X,y,nkeep,npred);
	


	% --------Fill up cell output
	cvout=[];  % no cross-validation output 
	recout{1}=yhat1;
	recout{2}=yhat2;
	recout{3}=rsq;
	recout{4}=[rmse v1]; % rmse for calib, validation
	recout{5}=v2; % RE
	recout{6}=NaN; % std error of estimate
	recout{7}=nv ; % this is 1 x 3
	recout{8}=hf;
   recout{10}=v3; % cross-validation residuals

end; % if kmode==1
